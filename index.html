<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Bench2ADVLM</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Bench2ADVLM:  Closed-Loop Evaluation Framework for Vision-language Models in Autonomous Driving</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <!--<span class="author-block">
                <a href="https://tianyuan2001.github.io/" target="_blank">Tianyuan Zhang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Ting Jin</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Lu Wang</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Jiangfan Liu</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://liangsiyuan21.github.io/" target="_blank">Siyuan Liang</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Mingchuan Zhang</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a href="https://liuaishan.github.io/index.html" target="_blank">Aishan Liu</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://xlliu-beihang.github.io/" target="_blank">Xianglong Liu</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=RwlJNLcAAAAJ&hl=en" target="_blank">Dacheng
                  Tao</a><sup>4</sup>
              </span>-->
            </div>

            <!--<div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup> Beihang University,</span>
              <span class="author-block"><sup>2</sup> National University of Singapore,</span>
              <span class="author-block"><sup>3</sup> Henan University of Science and Technology,</span>
              <span class="author-block"><sup>4</sup> Nanyang Technological University</span>
            </div>-->

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <!-- <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->

                <!-- Supplementary PDF link -->
                <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://anonymous.4open.science/r/BENCH2ADVLM-C0EF/README.md" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser video-->
  <!-- <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video poster="" id="tree" autoplay controls muted loop height="100%">
         
          <source src="static/videos/banner_video.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat
          pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus.
        </h2>
      </div>
    </div>
  </section> -->
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Vision-Language Models (VLMs) have recently emerged as a promising paradigm in autonomous driving (AD). However, current performance evaluation protocols for VLM-based AD systems (ADVLMs) are predominantly confined to open-loop settings with static inputs, neglecting the more realistic and informative closed-loop setting that captures interactive behavior, feedback resilience, and real-world safety. To address this, we introduce <b><i>Bench2ADVLM</i></b>, a unified hierarchical closed-loop evaluation framework for real-time, interactive assessment of ADVLMs across simulation platforms. Inspired by dual-process theories of cognition, we first adapt diverse ADVLMs to simulation environments via a dual-system adaptation architecture. In this design, heterogeneous high-level driving commands generated by target ADVLMs (fast system) are interpreted by a general-purpose VLM (slow system) into standardized control actions suitable for execution in simulation. To enable more comprehensive evaluation, <b><i>Bench2ADVLM</i></b> introduces a self-reflective scenario generation module that automatically explores model behavior and uncovers potential failure modes for safety-critical scenario generation, constructing a benchmark including 220 common routes and 220 threat scenarios. Experiments across 4 state-of-the-art ADVLMs and 16 different combinations validate the diagnostic strength of our framework, revealing that existing ADVLMs still exhibit limited performance under closed-loop conditions. Furthermore, we design a physical control abstraction layer that translates simulation actions into actuation signals, enabling closed-loop evaluation of ADVLMs on 3 physical vehicles. <b><i>Bench2ADVLM</i></b> is flexible and extensible, supporting diverse VLMs and enabling deployment across heterogeneous vehicles. To our knowledge, this is the first work to establish the closed-loop evaluation framework for ADVLMs, offering a principled path toward scalable, reliable deployment of ADVLMs.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->


  <section class="hero is-small">
    <div class="hero-body">
      <div class="container  is-max-desktop">
        <h2 class="title is-3">Overview</h2>
        <div class="level-set has-text-justified">
          <p style="font-size: large;">
            Overview of the <b><i>Bench2ADVLM</i></b> benchmark. The framework includes a dual-system adaptation
            architecture for translating high-level driving commands into mid-level control actions, a physical control
            abstraction layer for mapping mid-level control actions to low-level actuation signals, and a
            self-reflective scenario generation module for probing potential failure modes.
          </p><br>
          <img src="static/images/overview1.png" alt="overview" style="margin: 0 auto; display: block;">
          <h2 class="subtitle has-text-centered">
          </h2>
        </div>
      </div>
    </div>
  </section>

  <section class="hero is-small">
    <div class="hero-body">
      <div class="container  is-max-desktop">
        <h2 class="title is-3">Physical Control Abstraction Layer</h2>
        <div class="level-set has-text-justified">
          <p style="font-size: large;">
            Physical-world evaluation is performed on the AGILE·X sandbox using Jetbot and LIMO. Vehicles collect
            real-time data and send it to the dual-system, which produces high-level commands and mid-level actions.
            These are then translated by the abstraction layer into platform-specific low-level actuation signals,
            closing the control loop.
          </p><br>
          <img src="static/images/control.png" alt="control" style="margin: 0 auto; display: block;">
          <h2 class="subtitle has-text-centered">
          </h2>
        </div>
      </div>
    </div>
  </section>

  <section class="hero is-small">
    <div class="hero-body">
      <div class="container  is-max-desktop">
        <h2 class="title is-3">Self-reflective Scenario Generation</h2>
        <div class="level-set has-text-justified">
          <p style="font-size: large;">
            The ADVLM is prompted with P3 (perception–prediction–planning) queries, and a GVLM fuses the answers into a
            description.
          </p><br>
          <img src="static/images/scene.png" alt="scene" style="margin: 0 auto; display: block;  width: 60%;">
          <h2 class="subtitle has-text-centered">
          </h2>
        </div>
      </div>
    </div>
  </section>

  <section class="hero is-small">
    <div class="hero-body">
      <div class="container  is-max-desktop">
        <h2 class="title is-3">Main Experimental Results</h2>
        <div class="level-set has-text-justified">
          <p style="font-size: large;">
            Main experimental results on <b><i>Bench2ADVLM</i></b>. <sup>&dagger;</sup> and <sup>&star;</sup> indicate
            the use of Continuous Numerical Generation and Discrete Classification Selection parsing modes,
            respectively. Blue subscripts denote the standard deviation (<span style="color: blue;">±std</span>
            ) over multiple runs.
          </p><br>
          <img src="static/images/main1.png" alt="main" style="margin: 0 auto; display: block;">
          <h2 class="subtitle has-text-centered">
          </h2>
          <div class="level-set has-text-justified">
            <p style="font-size: large;">
              <b>Insight 1:</b> ADVLMs lack fine-grained control and show limited closed-loop performance, with low
              <em>Success Rate</em> and <em>Driving Score</em> highlighting a gap from deployment readiness.
            </p>
          </div>
        </div>
      </div>
  </section>

  <section class="hero is-small">
    <div class="hero-body">
      <div class="container  is-max-desktop">
        <h2 class="title is-3">Threat Scenario Evaluation</h2>
        <div class="level-set has-text-justified">
          <p style="font-size: large;">
            Experimental results on <b><i>Bench2ADVLM</i></b> under threat scenarios. Blue subscripts denote the
            standard deviation
            (<span style="color: blue;">±std</span>) over multiple runs, while red superscripts indicate the performance
            drop
            (<span style="color: red;">-drop</span>) compared to the main results.
          </p><br>
          <img src="static/images/threat1.png" alt="threat" style="margin: 0 auto; display: block;">
          <h2 class="subtitle has-text-centered">
          </h2>
          <p style="font-size: large;">
            Model performance over different scenarios on <b><i>Bench2ADVLM</i></b>.
          </p><br>
          <img src="static/images/threat2.png" alt="threat2" style="margin: 0 auto; display: block;">
          <h2 class="subtitle has-text-centered">
          </h2>
          <div class="level-set has-text-justified">
            <p style="font-size: large;">
              <b>Insight 2:</b> LLaVA shows a milder decline than LLaMA in behavior quality metrics (e.g.,
              <em>Efficiency</em>), while LLaMA performs better in basic performance (e.g., <em>Success Rate</em>).
            </p>
          </div>
        </div>
      </div>
  </section>

  <section class="hero is-small">
    <div class="hero-body">
      <div class="container  is-max-desktop">
        <h2 class="title is-3">Real-world Evaluation</h2>
        <div class="level-set has-text-justified">
          <p style="font-size: large;">
            We use two autonomous driving platforms: <a href="https://github.com/NVIDIA-AI-IOT/jetbot"
              target="_blank">Jetbot</a> and <a href="https://global.agilex.ai/products/limo-pro"
              target="_blank">LIMO</a>. Both platforms are equipped with onboard sensors, including cameras, LiDAR, and
            IMU, and are capable of standard motion control. Jetbot, featuring stronger onboard computational resources,
            is suited for AI-intensive workloads, while LIMO emphasizes actuation stability and supports multiple
            driving modes, including differential, Ackermann, tracked, and Mecanum configurations. Both platforms adopt
            ROS as the internal communication and control framework.
            The figure below showcases the real-world evaluation setup and representative experimental outcomes.
          </p><br>
          <img src="static/images/car1.png" alt="threat" style="margin: 0 auto; display: block;">
          <h2 class="subtitle has-text-centered">
          </h2>
        </div>
        <h2 class="subtitle has-text-centered">
        </h2>
        <div class="level-set has-text-justified">
          <p style="font-size: large;">
            To quantitatively assess the real-world driving performance, we design a structured evaluation strategy
            centered on the lane-following task.
            The driving sandbox is partitioned into ten distinct route segments, each reflecting varying geometric and
            traffic complexities.
            For each route, each AD vehicle runs three times, and we report the average results to ensure statistical
            reliability.
            The primary evaluation metric is the route completion rate, defined as the percentage of the planned
            trajectory successfully traversed by the vehicle without crossing the yellow boundary lines or colliding
            with obstacles.
          </p><br>
          <img src="static/images/real1.png" alt="threat" style="margin: 0 auto; display: block;">
          <h2 class="subtitle has-text-centered">
          </h2>
        </div>
        <h2 class="subtitle has-text-centered">
        </h2>
        <!-- <div class="level-set has-text-justified">
          <p style="font-size: large;">
            <b>Insight 3:</b> Insights from real-world evaluation.
          </p>
        </div> -->
      </div>
    </div>
    </div>
  </section>

  <section class="hero is-small">
    <div class="hero-body">
      <div class="container  is-max-desktop">
        <h2 class="title is-3">Visualization</h2>
        <div class="level-set has-text-justified">
          <p style="font-size: large;">
            First-person views captured every 0.5 seconds in <b>RouteScenario-3749</b>. The images show 6 frames per
            scene.
          </p><br>
          <img src="static/images/vis1.png" alt="threat" style="margin: 0 auto; display: block;">
          <h2 class="subtitle has-text-centered">
          </h2>
        </div>
        <h2 class="subtitle has-text-centered">
        </h2>
        <div class="level-set has-text-justified">
          <p style="font-size: large;">
            First-person views captured every 0.5 seconds in <b>RouteScenario-2082</b>. The images show 6 frames per
            scene.
          </p><br>
          <img src="static/images/vis2.png" alt="threat" style="margin: 0 auto; display: block;">
          <h2 class="subtitle has-text-centered">
          </h2>
        </div>
      </div>
  </section>

  <!-- Youtube video -->
  <!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
  <!-- End youtube video -->


  <!-- Video carousel -->
  <!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
  <!-- End video carousel -->






  <!-- Paper poster -->
  <!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
  <!--End paper poster -->


  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the source code of this website, we just ask that you link back to this page in the
              footer. <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>
