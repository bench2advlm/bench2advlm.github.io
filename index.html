<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Bench2ADVLM</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Bench2ADVLM: A Closed-Loop Benchmark for Vision-language Models in
              Autonomous Driving</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://tianyuan2001.github.io/" target="_blank">Tianyuan Zhang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Ting Jin</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Lu Wang</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Jiangfan Liu</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://liangsiyuan21.github.io/" target="_blank">Siyuan Liang</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Mingchuan Zhang</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a href="https://liuaishan.github.io/index.html" target="_blank">Aishan Liu</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://xlliu-beihang.github.io/" target="_blank">Xianglong Liu</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=RwlJNLcAAAAJ&hl=en" target="_blank">Dacheng
                  Tao</a><sup>4</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup> Beihang University,</span>
              <span class="author-block"><sup>2</sup> National University of Singapore,</span>
              <span class="author-block"><sup>3</sup> Henan University of Science and Technology,</span>
              <span class="author-block"><sup>4</sup> Nanyang Technological University</span>
              <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <!-- <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->

                <!-- Supplementary PDF link -->
                <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser video-->
  <!-- <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video poster="" id="tree" autoplay controls muted loop height="100%">
         
          <source src="static/videos/banner_video.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat
          pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus.
        </h2>
      </div>
    </div>
  </section> -->
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Vision-Language Models (VLMs) have recently emerged as a promising paradigm in autonomous driving (AD).
              However, current performance evaluation protocols for VLM-based AD systems (ADVLMs) are predominantly
              confined to open-loop settings with static inputs, neglecting the more realistic and informative
              closed-loop setting that captures interactive behavior, feedback resilience, and real-world safety. To
              address this, we introduce <b><i>Bench2ADVLM</i></b>, a unified hierarchical closed-loop evaluation
              framework for real-time, interactive assessment of ADVLMs across both simulation and physical platforms.
              Inspired by dual-process theories of cognition, we first adapt diverse ADVLMs to simulation environments
              via a dual-system adaptation architecture. In this design, heterogeneous high-level driving commands
              generated by target ADVLMs (fast system) are interpreted by a general-purpose VLM (slow system) into
              standardized mid-level control actions suitable for execution in simulation. To bridge the gap between
              simulation and reality, we design a physical control abstraction layer that translates these mid-level
              actions into low-level actuation signals, enabling, for the first time, closed-loop testing of ADVLMs on
              physical vehicles. To enable more comprehensive evaluation, <b><i>Bench2ADVLM</i></b> introduces a
              self-reflective scenario generation module that automatically explores model behavior and uncovers
              potential failure modes for safety-critical scenario generation. Overall, <b><i>Bench2ADVLM</i></b>
              establishes a hierarchical evaluation pipeline that seamlessly integrates high-level abstract reasoning,
              mid-level simulation actions, and low-level real-world execution. Experiments on diverse scenarios across
              multiple state-of-the-art ADVLMs and physical platforms validate the diagnostic strength of our framework,
              revealing that existing ADVLMs still exhibit limited performance under closed-loop conditions. To our
              knowledge, this is the first work to establish the closed-loop evaluation benchmark for ADVLMs, offering a
              principled path toward scalable, reliable deployment of ADVLMs.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->


  <section class="hero is-small">
    <div class="hero-body">
      <div class="container  is-max-desktop">
        <h2 class="title is-3">Overview</h2>
        <div class="level-set has-text-justified">
          <p style="font-size: large;">
            Overview of the <b><i>Bench2ADVLM</i></b> benchmark. The framework includes a dual-system adaptation
            architecture for translating high-level driving commands into mid-level control actions, a physical control
            abstraction layer for mapping mid-level control actions to low-level actuation signals, and a
            self-reflective scenario generation module for probing potential failure modes.
          </p><br>
          <img src="static/images/overview.png" alt="overview" style="margin: 0 auto; display: block;">
          <h2 class="subtitle has-text-centered">
          </h2>
        </div>
      </div>
    </div>
  </section>

  <section class="hero is-small">
    <div class="hero-body">
      <div class="container  is-max-desktop">
        <h2 class="title is-3">Physical Control Abstraction Layer</h2>
        <div class="level-set has-text-justified">
          <p style="font-size: large;">
            Physical-world evaluation is performed on the AGILE·X sandbox using Jetbot and LIMO. Vehicles collect
            real-time data and send it to the dual-system, which produces high-level commands and mid-level actions.
            These are then translated by the abstraction layer into platform-specific low-level actuation signals,
            closing the control loop.
          </p><br>
          <img src="static/images/control.png" alt="control" style="margin: 0 auto; display: block;">
          <h2 class="subtitle has-text-centered">
          </h2>
        </div>
      </div>
    </div>
  </section>

  <section class="hero is-small">
    <div class="hero-body">
      <div class="container  is-max-desktop">
        <h2 class="title is-3">Self-reflective Scenario Generation</h2>
        <div class="level-set has-text-justified">
          <p style="font-size: large;">
            The ADVLM is prompted with P3 (perception–prediction–planning) queries, and a GVLM fuses the answers into a
            description.
          </p><br>
          <img src="static/images/scene.png" alt="scene" style="margin: 0 auto; display: block;  width: 60%;">
          <h2 class="subtitle has-text-centered">
          </h2>
        </div>
      </div>
    </div>
  </section>

  <section class="hero is-small">
    <div class="hero-body">
      <div class="container  is-max-desktop">
        <h2 class="title is-3">Main Experimental Results</h2>
        <div class="level-set has-text-justified">
          <p style="font-size: large;">
            Main experimental results on <b><i>Bench2ADVLM</i></b>. <sup>&dagger;</sup> and <sup>&star;</sup> indicate
            the use of Continuous Numerical Generation and Discrete Classification Selection parsing modes,
            respectively.
          </p><br>
          <img src="static/images/main.png" alt="main" style="margin: 0 auto; display: block;">
          <h2 class="subtitle has-text-centered">
          </h2>
          <div class="level-set has-text-justified">
            <p style="font-size: large;">
              <b>Insight 1:</b> Insights from main results...
            </p>
          </div>
        </div>
      </div>
  </section>

  <section class="hero is-small">
    <div class="hero-body">
      <div class="container  is-max-desktop">
        <h2 class="title is-3">Threat Scenario Evaluation</h2>
        <div class="level-set has-text-justified">
          <p style="font-size: large;">
            The evaluation results of ADVLMs under threat-oriented scenarios, highlighting the robustness of current
            models when faced with rare but safety-critical driving conditions.
          </p><br>
          <img src="static/images/threat.png" alt="threat" style="margin: 0 auto; display: block;">
          <h2 class="subtitle has-text-centered">
          </h2>
          <div class="level-set has-text-justified">
            <p style="font-size: large;">
              <b>Insight 2:</b> Insights from hreat scenario evaluation.
            </p>
          </div>
        </div>
      </div>
  </section>

  <section class="hero is-small">
    <div class="hero-body">
      <div class="container  is-max-desktop">
        <h2 class="title is-3">Real-world Evaluation</h2>
        <div class="level-set has-text-justified">
          <p style="font-size: large;">
            We use two autonomous driving platforms: <a href="https://github.com/NVIDIA-AI-IOT/jetbot"
              target="_blank">Jetbot</a> and <a href="https://global.agilex.ai/products/limo-pro"
              target="_blank">LIMO</a>. Both platforms are equipped with onboard sensors, including cameras, LiDAR, and
            IMU, and are capable of standard motion control. Jetbot, featuring stronger onboard computational resources,
            is suited for AI-intensive workloads, while LIMO emphasizes actuation stability and supports multiple
            driving modes, including differential, Ackermann, tracked, and Mecanum configurations. Both platforms adopt
            ROS as the internal communication and control framework.
            The figure above showcases the JetBot, while the one below presents the LIMO platform.
          </p><br>
          <img src="static/images/car.png" alt="threat" style="margin: 0 auto; display: block;">
          <h2 class="subtitle has-text-centered">
          </h2>
        </div>
        <h2 class="subtitle has-text-centered">
        </h2>
        <div class="level-set has-text-justified">
          <p style="font-size: large;">
            The driving sandbox is partitioned into five distinct route segments, each reflecting varying geometric and
            traffic complexities.
            For each route, each autonomous driving vehicle runs three times, and we report the average results to
            ensure statistical reliability.
            The primary evaluation metric is the route completion rate, defined as the percentage of the planned
            trajectory successfully traversed by the vehicle without crossing the yellow boundary lines, colliding with
            obstacles, or violating traffic signals.
            This metric effectively captures the agent’s ability to maintain robust and rule-compliant navigation under
            real-world constraints.
          </p><br>
          <img src="static/images/real.png" alt="threat" style="margin: 0 auto; display: block;">
          <h2 class="subtitle has-text-centered">
          </h2>
        </div>
        <h2 class="subtitle has-text-centered">
        </h2>
        <div class="level-set has-text-justified">
          <p style="font-size: large;">
            <b>Insight 3:</b> Insights from real-world evaluation.
          </p>
        </div>
      </div>
    </div>
    </div>
  </section>

  <section class="hero is-small">
    <div class="hero-body">
      <div class="container  is-max-desktop">
        <h2 class="title is-3">Visualization</h2>
        <div class="level-set has-text-justified">
          <p style="font-size: large;">
            First-person views captured every 0.5 seconds in <b>RouteScenario-2082</b>. The images show 6 frames per
            scene.
          </p><br>
          <img src="static/images/vis1.png" alt="threat" style="margin: 0 auto; display: block;">
          <h2 class="subtitle has-text-centered">
          </h2>
        </div>
        <h2 class="subtitle has-text-centered">
        </h2>
        <div class="level-set has-text-justified">
          <p style="font-size: large;">
            First-person views captured every 0.5 seconds in <b>RouteScenario-3749</b>. The images show 6 frames per
            scene.
          </p><br>
          <img src="static/images/vis2.png" alt="threat" style="margin: 0 auto; display: block;">
          <h2 class="subtitle has-text-centered">
          </h2>
        </div>
      </div>
  </section>

  <!-- Youtube video -->
  <!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
  <!-- End youtube video -->


  <!-- Video carousel -->
  <!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
  <!-- End video carousel -->






  <!-- Paper poster -->
  <!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
  <!--End paper poster -->


  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the source code of this website, we just ask that you link back to this page in the
              footer. <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>